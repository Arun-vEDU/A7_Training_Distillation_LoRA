# A7_Training_Distillation_LoRA

## Compare the performance of LoRA with the Odd Layer and Even Layer initializations

Note: I run this experiment on Google co-labs with 1% datafrom traning set.

| Model Type | Training Loss | Test Set Performance |  
|------------|--------------|----------------------|
| Odd Layer   | 0.1613      | 94.16%               | 
| Even Layer    |1618        | 93.74%               | 
| LoRA     | 0.1732      | 92.58%               |

## Evaluation and Analysis

| Model Type | Training Loss | Test Set Performance |  
|------------|--------------|----------------------|
| Odd Layer   | 0.1385        | 92.30%               | 
| Even Layer    | 0.1411        | 93.74%               | 
| LoRA     | 0.1732        | 92.58%               |

## Demo video
[![Watch the video](https://img.youtube.com/vi/tpJAWBjJGdY/maxresdefault.jpg)](https://youtu.be/tpJAWBjJGdY)
