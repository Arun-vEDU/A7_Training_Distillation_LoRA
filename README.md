# A7_Training_Distillation_LoRA

## Compare the performance of LoRA with the Odd Layer and Even Layer initializations

Note: I run this experiment on Google co-labs with 1% datafrom traning set.

| Model Type | Training Loss | Test Set Performance |  
|------------|--------------|----------------------|
| Odd Layer   | 0        | 94.16%               | 
| Even Layer    | 0        | 93.74%               | 
| LoRA     | XX.XX        | 92.58%               |

## Evaluation and Analysis

| Model Type | Training Loss | Test Set Performance |  
|------------|--------------|----------------------|
| Odd Layer   | 0.1385        | 92.30%               | 
| Even Layer    | 0.1411        | 93.74%               | 
| LoRA     | XX.XX        | 92.58%               |
