{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id    target                                       comment_text  \\\n",
      "458232   806064  0.000000  It's difficult for many old people to keep up ...   \n",
      "272766   576402  0.166667  She recognized that her tiny-handed husband is...   \n",
      "339129   658508  0.000000  HPHY76,\\nGood for you for thinking out loud, w...   \n",
      "773565  5066714  0.500000  And I bet that in the day you expected your Je...   \n",
      "476233   828147  0.000000  Kennedy will add a much needed and scientifica...   \n",
      "\n",
      "        severe_toxicity  obscene  identity_attack    insult  threat  asian  \\\n",
      "458232              0.0      0.0              0.0  0.000000     0.0    NaN   \n",
      "272766              0.0      0.0              0.0  0.166667     0.0    NaN   \n",
      "339129              0.0      0.0              0.0  0.000000     0.0    NaN   \n",
      "773565              0.0      0.0              0.4  0.100000     0.0    0.0   \n",
      "476233              0.0      0.0              0.0  0.000000     0.0    NaN   \n",
      "\n",
      "        atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
      "458232      NaN  ...      160581  approved      0    0    0      1         0   \n",
      "272766      NaN  ...      150555  approved      0    0    0      2         0   \n",
      "339129      NaN  ...      154368  approved      0    0    0      0         0   \n",
      "773565      0.0  ...      322772  approved      0    0    0      4         2   \n",
      "476233      NaN  ...      161073  approved      0    0    1      0         0   \n",
      "\n",
      "        sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
      "458232              0.0                         0                         4  \n",
      "272766              0.0                         0                         6  \n",
      "339129              0.0                         0                         4  \n",
      "773565              0.0                         6                        10  \n",
      "476233              0.0                         0                         4  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "train_df_full = pd.read_csv('train.csv')  # Replace with the actual path to your dataset\n",
    "test_df_full = pd.read_csv('test.csv')    # Replace with the actual path to your dataset\n",
    "\n",
    "# Take 1% of the training dataset and assign it to train_df\n",
    "train_df= train_df_full.sample(frac=0.01, random_state=SEED)  # random_state ensures reproducibility\n",
    "\n",
    "# Take 1% of the test dataset and assign it to test_df\n",
    "test_df= test_df_full.sample(frac=0.01, random_state=SEED)  # random_state ensures reproducibility\n",
    "\n",
    "# Display the first few rows\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
      "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
      "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
      "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
      "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
      "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
      "       'other_sexual_orientation', 'physical_disability',\n",
      "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
      "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
      "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
      "       'identity_annotator_count', 'toxicity_annotator_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the column names\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0.000000    12729\n",
      "0.166667     1421\n",
      "0.200000     1055\n",
      "0.300000      551\n",
      "0.400000      510\n",
      "            ...  \n",
      "0.506667        1\n",
      "0.876923        1\n",
      "0.148148        1\n",
      "0.770492        1\n",
      "0.095238        1\n",
      "Name: count, Length: 458, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    16677\n",
      "1     1372\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    " #Handle missing or invalid values in the 'comment_text' column\n",
    "train_df['comment_text'] = train_df['comment_text'].fillna('')  # Replace NaN with empty strings\n",
    "train_df['comment_text'] = train_df['comment_text'].astype(str)  # Ensure all values are strings\n",
    "\n",
    "# Preprocess the text (example: lowercase and remove special characters)\n",
    "train_df['comment_text'] = train_df['comment_text'].str.lower().replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# Apply a threshold of 0.5 to binarize the 'target' column\n",
    "train_df['target'] = (train_df['target'] >= 0.5).astype(int)\n",
    "\n",
    "# Check the distribution of the binarized labels\n",
    "print(train_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training texts: 14439\n",
      "Validation texts: 3610\n",
      "Training labels distribution:\n",
      " target\n",
      "0    13340\n",
      "1     1099\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['comment_text'], train_df['target'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Verify the split\n",
    "print(\"Training texts:\", len(train_texts))\n",
    "print(\"Validation texts:\", len(val_texts))\n",
    "print(\"Training labels distribution:\\n\", train_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afe22bdb1434523a2f736671181ae59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14439 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c91879e5444ed7ab1b0127d0f08e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')  # Replace with your model ID if different\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'labels': train_labels.astype(int)})  # Ensure labels are integers\n",
    "val_dataset = Dataset.from_dict({'text': val_texts, 'labels': val_labels.astype(int)})  # Ensure labels are integers\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertConfig, BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_train_dataset, shuffle=True, batch_size=32, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(tokenized_val_dataset, batch_size=32, collate_fn=data_collator)\n",
    "\n",
    "# Define the Teacher Model\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "teacher_model.to(device)\n",
    "teacher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student model initialized with odd layers.\n"
     ]
    }
   ],
   "source": [
    "# Define the Student Model\n",
    "configuration = teacher_model.config.to_dict()\n",
    "configuration['num_hidden_layers'] //= 2  # Reduce the number of layers by half\n",
    "student_config = BertConfig.from_dict(configuration)\n",
    "student_model = AutoModelForSequenceClassification.from_config(student_config)\n",
    "student_model.to(device)\n",
    "\n",
    "# Function to Distill Weights (Odd Layers)\n",
    "def distill_odd_layers(teacher, student):\n",
    "    teacher_layers = teacher.bert.encoder.layer  # Access the encoder layers of the teacher\n",
    "    student_layers = student.bert.encoder.layer  # Access the encoder layers of the student\n",
    "\n",
    "    for i in range(len(student_layers)):\n",
    "        # Copy weights from odd layers of the teacher (1, 3, 5, 7, 9, 11)\n",
    "        student_layers[i].load_state_dict(teacher_layers[2 * i].state_dict())\n",
    "\n",
    "# Function to Distill Weights (Even Layers)\n",
    "def distill_even_layers(teacher, student):\n",
    "    teacher_layers = teacher.bert.encoder.layer  # Access the encoder layers of the teacher\n",
    "    student_layers = student.bert.encoder.layer  # Access the encoder layers of the student\n",
    "\n",
    "    for i in range(len(student_layers)):\n",
    "        # Copy weights from even layers of the teacher (2, 4, 6, 8, 10, 12)\n",
    "        student_layers[i].load_state_dict(teacher_layers[2 * i + 1].state_dict())\n",
    "\n",
    "# Distill Weights (Odd Layers)\n",
    "distill_odd_layers(teacher_model, student_model)\n",
    "print(\"Student model initialized with odd layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Functions\n",
    "criterion_div = nn.KLDivLoss(reduction='batchmean')\n",
    "criterion_cos = nn.CosineEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer and Scheduler\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d46a2570934e6d8833921ef4922e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch at 1: Train loss 0.1635:\n",
      "  - Loss_cls: 0.3394\n",
      "  - Loss_div: 0.0656\n",
      "  - Loss_cos: 0.0855\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc4cde22c164e8392299c06e2853ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch at 2: Train loss 0.1544:\n",
      "  - Loss_cls: 0.3200\n",
      "  - Loss_div: 0.0765\n",
      "  - Loss_cos: 0.0666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48ab91f29b34fac9387b8d0349cd1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch at 3: Train loss 0.1385:\n",
      "  - Loss_cls: 0.2603\n",
      "  - Loss_div: 0.0918\n",
      "  - Loss_cos: 0.0635\n"
     ]
    }
   ],
   "source": [
    "# Lists to store losses for plotting\n",
    "train_losses = []\n",
    "train_losses_cls = []\n",
    "train_losses_div = []\n",
    "train_losses_cos = []\n",
    "eval_losses = []\n",
    "eval_accuracies = []\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "    total_loss = 0\n",
    "    total_loss_cls = 0\n",
    "    total_loss_div = 0\n",
    "    total_loss_cos = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass (Student)\n",
    "        student_outputs = student_model(**batch)\n",
    "\n",
    "        # Forward pass (Teacher)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(**batch)\n",
    "\n",
    "        # Compute Losses\n",
    "        loss_cls = student_outputs.loss  # Classification loss\n",
    "        loss_div = criterion_div(\n",
    "            torch.log_softmax(student_outputs.logits / 2.0, dim=-1),\n",
    "            torch.softmax(teacher_outputs.logits / 2.0, dim=-1)\n",
    "        )  # Distillation loss\n",
    "        loss_cos = criterion_cos(\n",
    "            student_outputs.logits, teacher_outputs.logits, torch.ones(batch['input_ids'].size(0)).to(device)\n",
    "        )  # Cosine loss\n",
    "\n",
    "        # Total Loss\n",
    "        loss = (loss_cls + loss_div + loss_cos) / 3\n",
    "        total_loss += loss.item()\n",
    "        total_loss_cls += loss_cls.item()\n",
    "        total_loss_div += loss_div.item()\n",
    "        total_loss_cos += loss_cos.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Store losses for plotting\n",
    "    train_losses.append(total_loss / len(train_dataloader))\n",
    "    train_losses_cls.append(total_loss_cls / len(train_dataloader))\n",
    "    train_losses_div.append(total_loss_div / len(train_dataloader))\n",
    "    train_losses_cos.append(total_loss_cos / len(train_dataloader))\n",
    "\n",
    "    # Print training logs\n",
    "    print(f'Epoch at {epoch + 1}: Train loss {train_losses[-1]:.4f}:')\n",
    "    print(f'  - Loss_cls: {train_losses_cls[-1]:.4f}')\n",
    "    print(f'  - Loss_div: {train_losses_div[-1]:.4f}')\n",
    "    print(f'  - Loss_cos: {train_losses_cos[-1]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch at 3: Test Acc 0.9230\n",
      "Avg Metric 0.3076638965835642\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Plotting\u001b[39;00m\n\u001b[0;32m     29\u001b[0m epochs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     32\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs_list, train_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Train Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs_list, train_losses_cls, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss_cls\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Evaluation\n",
    "student_model.eval()\n",
    "eval_loss = 0\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = student_model(**batch)\n",
    "        loss_cls = outputs.loss\n",
    "        eval_loss += loss_cls.item()\n",
    "\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        total_correct += (predictions == batch['labels']).sum().item()\n",
    "        total_samples += batch['labels'].size(0)\n",
    "\n",
    "    eval_losses.append(eval_loss / len(val_dataloader))\n",
    "    eval_accuracy = total_correct / total_samples\n",
    "    eval_accuracies.append(eval_accuracy)\n",
    "\n",
    "    # Print evaluation logs\n",
    "    print(f\"Epoch at {epoch + 1}: Test Acc {eval_accuracy:.4f}\")  # Correctly indented\n",
    "\n",
    "# Print average evaluation metric\n",
    "print('Avg Metric', sum(eval_accuracies) / num_epochs)\n",
    "\n",
    "# Plotting\n",
    "epochs_list = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs_list, train_losses, label='Total Train Loss')\n",
    "plt.plot(epochs_list, train_losses_cls, label='Train Loss_cls')\n",
    "plt.plot(epochs_list, train_losses_div, label='Train Loss_div')\n",
    "plt.plot(epochs_list, train_losses_cos, label='Train Loss_cos')\n",
    "plt.plot(epochs_list, eval_losses, label='Validation Loss')\n",
    "\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
